{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalize Sample with Database\n",
    "\n",
    "The goal of this notebook is to show how to extract data from a database in a batch process and to inject it into Amazon Personalize in order to generate recommendations. \n",
    "\n",
    "This process will use the Django ORM to query the database as well as Pandas for manipulating the query results into usable CSVs for Amazon Personalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Personalize Into Boto3\n",
    "\n",
    "Given that Amazon Personalize is a service in preview, the boto3 library and AWS CLI must be updated manually in order to allow calls to be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-11 20:34:37--  https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize.json\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.225.88\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.225.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘personalize.json’ not modified on server. Omitting download.\n",
      "\n",
      "--2019-02-11 20:34:37--  https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize-runtime.json\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.225.88\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.225.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘personalize-runtime.json’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "!wget -N https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize.json\n",
    "!wget -N https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize-runtime.json\n",
    "!aws configure add-model --service-model file://`pwd`/personalize.json --service-name personalize\n",
    "!aws configure add-model --service-model file://`pwd`/personalize-runtime.json --service-name personalize-runtime\n",
    "\n",
    "personalize = boto3.client(service_name='personalize', endpoint_url='https://personalize.us-east-1.amazonaws.com')\n",
    "personalize_runtime = boto3.client(service_name='personalize-runtime', endpoint_url='https://personalize-runtime.us-east-1.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Bucket and Data Output Location\n",
    "\n",
    "You will need to create an S3 bucket in order to store all of your data for Amazon Personalize.\n",
    "\n",
    "Create a bucket by opening a new tab and visiting: https://s3.console.aws.amazon.com/s3/home?region=us-east-1#\n",
    "\n",
    "Once there click the `Create Bucket` button. You will then be prompted for a unique name, please choose one. It is recommended to use something relevant to the lab like `first_namedjangopersonalize` then click `Next`. Click `Next` again. All of the options that are default are fine so click `Next` once more. The same goes for permissions so click `Create Bucket` and note the name of the bucket you created. You will update the cell below with that value.\n",
    "\n",
    "Once the bucket has been created, continue on with the process below\n",
    "\n",
    "Next specify the bucket that you will be using for your own data, this is where you will initially upload CSV copies of the data extracted from your database.\n",
    "\n",
    "The filenames should reflect their role in the Personalize process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"djangopersonalizedemo\"           # replace with the name of your S3 bucket\n",
    "\n",
    "\n",
    "user_metadata = \"user_metadata.csv\"\n",
    "item_metadata = \"item_metadata.csv\"\n",
    "user_interaction = \"user_interaction.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export, Prepare, and Upload Training Data\n",
    "\n",
    "First in this process we will export the user_metadata items and place them in a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Django and Pandas\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import django\n",
    "django.setup()\n",
    "\n",
    "from movielens.models import User\n",
    "from movielens.models import Item\n",
    "from movielens.models import UserData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>OCCUPATION</th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ZIP_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>1</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>3</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>4</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>5</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGE GENDER  OCCUPATION  USER_ID ZIP_CODE\n",
       "0   24      M  technician        1    85711\n",
       "1   53      F       other        2    94043\n",
       "2   23      M      writer        3    32067\n",
       "3   24      M  technician        4    43537\n",
       "4   33      F       other        5    15213"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = pd.DataFrame(list(User.objects.all().values('user_id', 'age','gender','occupation','zip_code')))\n",
    "pd.set_option('display.max_rows', 5)\n",
    "user_df.columns = ['AGE', 'GENDER', 'OCCUPATION', 'USER_ID', 'ZIP_CODE']\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.to_csv(user_metadata, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(user_metadata).upload_file(user_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>MOVIE_TITLE</th>\n",
       "      <th>RELEASE_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ITEM_ID        MOVIE_TITLE RELEASE_DATE\n",
       "0        1   Toy Story (1995)  01-Jan-1995\n",
       "1        2   GoldenEye (1995)  01-Jan-1995\n",
       "2        3  Four Rooms (1995)  01-Jan-1995\n",
       "3        4  Get Shorty (1995)  01-Jan-1995\n",
       "4        5     Copycat (1995)  01-Jan-1995"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df = pd.DataFrame(list(Item.objects.all().values('movie_id', 'movie_title', 'release_date')))\n",
    "item_df.columns\n",
    "new_columns = []\n",
    "for item in item_df.columns:\n",
    "    new_columns.append(item.upper())\n",
    "item_df.columns = new_columns\n",
    "item_df=item_df.rename(columns = {'MOVIE_ID':'ITEM_ID'})\n",
    "item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.to_csv(item_metadata, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(item_metadata).upload_file(item_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload User Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>474</td>\n",
       "      <td>4</td>\n",
       "      <td>884182806</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>891628467</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55373</th>\n",
       "      <td>538</td>\n",
       "      <td>4</td>\n",
       "      <td>892685437</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55374</th>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>879795543</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ITEM_ID  RATING  TIMESTAMP  USER_ID\n",
       "0          474       4  884182806      298\n",
       "1          465       5  891628467      253\n",
       "...        ...     ...        ...      ...\n",
       "55373      538       4  892685437      676\n",
       "55374      204       5  879795543      716\n",
       "\n",
       "[55375 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note we are using the Django Queryset functionality to select ratings only >= 3.6\n",
    "interaction_df = pd.DataFrame(list(UserData.objects.filter(rating__gte=3.6)\n",
    "                                   .values('user_id', 'item_id', 'rating', 'timestamp')))\n",
    "interaction_df.columns = ['ITEM_ID', 'RATING', 'TIMESTAMP', 'USER_ID']\n",
    "\n",
    "interaction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df.to_csv(user_interaction, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(user_interaction).upload_file(user_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schema\n",
    "\n",
    "The next large step will be creating schemas for all 3 files and then placing them inside the Personalize service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Interaction Schema First\n",
    "\n",
    "This is required to make Personalize function so we will start with the last data exported. If you get an error that the resource already exists, change the name variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:059124553121:schema/interactions-schemadjd\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"de8c7937-967b-4201-b1f4-e1c80db7a9c5\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:35:05 GMT\",\n",
      "      \"x-amzn-requestid\": \"de8c7937-967b-4201-b1f4-e1c80db7a9c5\",\n",
      "      \"content-length\": \"88\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"interactions-schemadjd\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-group/personalens-dataset-group\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"386d2038-8821-45aa-812b-d39d5b496daa\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:35:10 GMT\",\n",
      "      \"x-amzn-requestid\": \"386d2038-8821-45aa-812b-d39d5b496daa\",\n",
      "      \"content-length\": \"104\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"personalens-dataset-group\"\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset/personalens-dataset-group/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c6b1db21-26e5-4a9f-9279-caa201f5ad54\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:36:21 GMT\",\n",
      "      \"x-amzn-requestid\": \"c6b1db21-26e5-4a9f-9279-caa201f5ad54\",\n",
      "      \"content-length\": \"106\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare, Create, and Wait for Dataset Import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach policy to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Read Only Access Role\n",
    "\n",
    "Note you pay need to update the role_name if you get a notice that it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::059124553121:role/PersonalizeS3RoleDjangoF\n"
     ]
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeS3RoleDjangoF\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ");\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ");\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-import-job/django-dataset-import-jobI\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"3b60419c-a585-4fa4-951f-6706eb4e5c9f\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:37:32 GMT\",\n",
      "      \"x-amzn-requestid\": \"3b60419c-a585-4fa4-951f-6706eb4e5c9f\",\n",
      "      \"content-length\": \"114\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"django-dataset-import-jobI\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, user_interaction)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Import Job and Dataset Import Job Run to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import User Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:059124553121:schema/django-users-schemaF\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"8195882c-a738-4cb9-95ba-b6c01fa690cb\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:52:46 GMT\",\n",
      "      \"x-amzn-requestid\": \"8195882c-a738-4cb9-95ba-b6c01fa690cb\",\n",
      "      \"content-length\": \"86\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ['AGE', 'GENDER', 'OCCUPATION', 'USER_ID', 'ZIP_CODE']\n",
    "user_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"AGE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"OCCUPATION\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ZIP_CODE\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"django-users-schemaF\",\n",
    "    schema = json.dumps(user_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given that the dataset group was already created, we will continue to use that but updating the dataset type below, then uploading our data into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset/personalens-dataset-group/USERS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"8599f41c-feb8-4ffc-bb40-595e1c926c63\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:52:51 GMT\",\n",
      "      \"x-amzn-requestid\": \"8599f41c-feb8-4ffc-bb40-595e1c926c63\",\n",
      "      \"content-length\": \"99\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"USERS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-import-job/django-import-usersF\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"56789edc-71a6-4f10-a904-3ef3f87b61f7\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 20:52:57 GMT\",\n",
      "      \"x-amzn-requestid\": \"56789edc-71a6-4f10-a904-3ef3f87b61f7\",\n",
      "      \"content-length\": \"108\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"django-import-usersF\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, user_metadata)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Item Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:059124553121:schema/django-items-schema-finalF\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"81a1d44d-1a80-466b-8c95-bca2a5de5e7e\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 21:34:40 GMT\",\n",
      "      \"x-amzn-requestid\": \"81a1d44d-1a80-466b-8c95-bca2a5de5e7e\",\n",
      "      \"content-length\": \"92\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "item_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Item\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MOVIE_TITLE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RELEASE_DATE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"django-items-schema-finalF\",\n",
    "    schema = json.dumps(item_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset/personalens-dataset-group/ITEMS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"a547fba4-4f75-4af1-b450-8e0ea82ef7b5\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 21:34:48 GMT\",\n",
      "      \"x-amzn-requestid\": \"a547fba4-4f75-4af1-b450-8e0ea82ef7b5\",\n",
      "      \"content-length\": \"99\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"ITEMS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-import-job/django-import-items-finalF\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"8f3325f2-176a-47f8-ba23-abcd4a1edc63\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 21:35:00 GMT\",\n",
      "      \"x-amzn-requestid\": \"8f3325f2-176a-47f8-ba23-abcd4a1edc63\",\n",
      "      \"content-length\": \"114\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"django-import-items-finalF\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, item_metadata)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:::recipe/awspersonalizehrnnmodel\n"
     ]
    }
   ],
   "source": [
    "recipe_list = [\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizehrnnmodel\",\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizedeepfmmodel\",\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizesimsmodel\",\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizeffnnmodel\",\n",
    "    \"arn:aws:personalize:::recipe/popularity-baseline\"\n",
    "]\n",
    "\n",
    "recipe_arn = recipe_list[0]\n",
    "print(recipe_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:059124553121:solution/Dj-movielens-soln\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"93eff6ca-c227-4e8b-a6b7-a8235c488d43\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 21:53:33 GMT\",\n",
      "      \"x-amzn-requestid\": \"93eff6ca-c227-4e8b-a6b7-a8235c488d43\",\n",
      "      \"content-length\": \"87\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_solution_response = personalize.create_solution(\n",
    "    name = \"Dj-movielens-soln\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = recipe_arn,\n",
    "    minProvisionedTPS = 1\n",
    ")\n",
    "\n",
    "solution_arn = create_solution_response['solutionArn']\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Solution to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: CREATE PENDING\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_solution_response = personalize.describe_solution(\n",
    "        solutionArn = solution_arn\n",
    "    )\n",
    "    status = describe_solution_response[\"solution\"][\"status\"]\n",
    "    print(\"Solution: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Metrics of Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metrics\": {\n",
      "    \"arn:aws:personalize:us-east-1:059124553121:model/awspersonalizehrnnmodel-eecea2ab\": {\n",
      "      \"_num_evaluation_users\": 91.0,\n",
      "      \"_num_unique_items\": 1448.0,\n",
      "      \"_user_history_length_10_pct_quantile\": 12.0,\n",
      "      \"_user_history_length_50_pct_quantile\": 34.0,\n",
      "      \"_user_history_length_90_pct_quantile\": 140.0,\n",
      "      \"_user_history_length_mean\": 53.18681318681319,\n",
      "      \"coverage\": 0.2776243093922652,\n",
      "      \"mean_reciprocal_rank\": 0.025642051692471864,\n",
      "      \"normalized_discounted_cumulative_gain_at_10\": 0.045510255107105546,\n",
      "      \"normalized_discounted_cumulative_gain_at_25\": 0.065279773604059,\n",
      "      \"normalized_discounted_cumulative_gain_at_5\": 0.02671073140739992,\n",
      "      \"precision_at_10\": 0.00989010989010989,\n",
      "      \"precision_at_25\": 0.007032967032967033,\n",
      "      \"precision_at_5\": 0.008791208791208791\n",
      "    }\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"26a6f26a-b8f3-4bc7-9afa-69a0a769e324\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 23:07:54 GMT\",\n",
      "      \"x-amzn-requestid\": \"26a6f26a-b8f3-4bc7-9afa-69a0a769e324\",\n",
      "      \"content-length\": \"719\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "get_metrics_response = personalize.get_metrics(\n",
    "    solutionArn = solution_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(get_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"campaignArn\": \"arn:aws:personalize:us-east-1:059124553121:campaign/Dj-campaign\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"387c1b3a-1256-40d8-995e-8f4b5275fb0d\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Mon, 11 Feb 2019 23:07:57 GMT\",\n",
      "      \"x-amzn-requestid\": \"387c1b3a-1256-40d8-995e-8f4b5275fb0d\",\n",
      "      \"content-length\": \"81\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_campaign_response = personalize.create_campaign(\n",
    "    name = \"Dj-campaign\",\n",
    "    solutionArn = solution_arn,\n",
    "    updateMode = \"MANUAL\"\n",
    ")\n",
    "\n",
    "campaign_arn = create_campaign_response['campaignArn']\n",
    "print(json.dumps(create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture the campaignARN\n",
    "\n",
    "In the above response you will see a campaign arn like `arn:aws:personalize:us-east-1:059124553121:campaign/Dj-campaign` save this for use when you go back to the application side. That is how you will communicate with your model in order to generate recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Campaign to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign: CREATE PENDING\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn\n",
    "    )\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    print(\"Campaign: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call GetRecommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: \n",
      "Marvin's Room (1996)\n",
      "3 Ninjas: High Noon At Mega Mountain (1998)\n",
      "Evita (1996)\n",
      "Full Monty, The (1997)\n",
      "Starship Troopers (1997)\n",
      "George of the Jungle (1997)\n",
      "Wag the Dog (1997)\n",
      "Fierce Creatures (1997)\n",
      "Game, The (1997)\n",
      "In & Out (1997)\n",
      "MatchMaker, The (1997)\n",
      "Dark City (1998)\n",
      "Donnie Brasco (1997)\n",
      "Heat (1995)\n",
      "L.A. Confidential (1997)\n",
      "Wings of the Dove, The (1997)\n",
      "As Good As It Gets (1997)\n",
      "Soul Food (1997)\n",
      "Event Horizon (1997)\n",
      "Replacement Killers, The (1998)\n",
      "U Turn (1997)\n",
      "Fly Away Home (1996)\n",
      "One Night Stand (1997)\n",
      "Cop Land (1997)\n",
      "Devil's Advocate, The (1997)\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = str(155),\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Recommendations: \")\n",
    "item_list = get_recommendations_response['itemList']\n",
    "for item in item_list:\n",
    "    print(item_df.iloc[int(item['itemId'])]['MOVIE_TITLE'])\n",
    "#print(\"Recommendations: {}\".format(json.dumps(title_list, indent=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up Here\n",
    "\n",
    "Now that you have completed this notebook it is time to go back to the documentation, specifically the `IntegratingAmazonPersonalizeWithDjango.md` file.\n",
    "It will be linked to in the page that referred you to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
